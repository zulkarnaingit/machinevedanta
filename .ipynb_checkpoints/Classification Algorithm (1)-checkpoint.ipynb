{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5f0aa8d6",
   "metadata": {},
   "source": [
    "To balance the data when the data is unbalanced, we use sampling technique\n",
    "First we need to install the package pip install imblearn\n",
    "\n",
    "To import RandomOverSampling this class we need to call it from imblearn inbuilt package, it holds the class over_sampling and import   RandomOverSampler.\n",
    "\n",
    "To importTo import RandomUnderSampling this class we need to call it from imblearn inbuilt package, it holds the class under_sampling and import   RandomUnderSampler.\n",
    "\n",
    "There are 2 types of Sampling Technique\n",
    "1.RandomOverSampling\n",
    "2.RandomUnderSampling\n",
    "\n",
    "1.RandomOverSampling:-It means it will automatically add the value to 1\n",
    "        It will make the count same for 0's and 1's\n",
    "        for eg:- if 0=500 and 1=200 \n",
    "        as we can see the data here is unbalanced so RandomOverSampling will add the data to 1 by duplicating its own data\n",
    "       \n",
    "2.RandomUnderSampling:-It will automatically remove certain data from another column to balance it out\n",
    "        It will make the count same for 0's and 1's\n",
    "        for eg:- if 0=500 and 1=200\n",
    "        as we can see the data here is unbalanced so RandomUnderSampling will remove certain data from 0 column to balance\n",
    "        the data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2457adca",
   "metadata": {},
   "source": [
    "Classficiation Algorithm : It means to predict the categorical type of data.\n",
    "Categorical type means give the category.\n",
    "There are 2 types of classification:-\n",
    "1.Binary classification:- It means there are 2 classes in it 0 0r 1, true or false, yes or no\n",
    "2.Multiclass classification:- There are more than 2 classes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "583137d5",
   "metadata": {},
   "source": [
    "There are different types of classification alogrithm:-\n",
    "1.Logistic regression\n",
    "2.Decision tree classifier\n",
    "3.support vector machine\n",
    "4.KNN means K nearest neighbours algorithm\n",
    "5.Naive bayes theorem\n",
    "6.Ensembling Technique\n",
    "    A.Random Forest Tree (bagging)\n",
    "    There are 2 types of Bootstraping method: 1.bagging and 2. pasting\n",
    "    B.Boosting Techniques\n",
    "        1.ADA Boost(Adaptor Boosting)\n",
    "        2.Gradient Boosting\n",
    "        3.Extream Gradient Boosting(XGB)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "835d15f8",
   "metadata": {},
   "source": [
    "LOGISTICS REGRESSION:- It will predict the output in the form of 0's and 1's\n",
    "    -We will perform this with the help of Probability\n",
    "    -The threshold value of probability is 0.5. if the value is below 0.5 it will go the value towards 0 and above 0.5 will go towards 1\n",
    "    -It will give an 'S' shaped curve, it is also known as Sigmoid Function\n",
    "    -Formula:-y=1/1+e^-x\n",
    "        e=exponent\n",
    "        ^=power \n",
    "        x=input\n",
    "        y=output\n",
    "    -It will return the probability value\n",
    "Limitation of Logistic Regression: This algorithm cannot select important features automatic in given dataset."
   ]
  },
  {
   "cell_type": "raw",
   "id": "89053a15",
   "metadata": {},
   "source": [
    "2.DecisionTreeClassifier: It is second classification Algorithm\n",
    "with the help of DecisionTreeClassifier, select important features from given dataset.\n",
    "By default it will use Gini Index to find the error\n",
    "\n",
    "There are 2 types of method used in DecisionTreeClassifier Algorithm:\n",
    "1.Gini Index/gini impurity\n",
    "2.Entropy(It is also known as loss function/error/cost function)\n",
    "\n",
    "Formula for GiniIndex = 1-P^2-Q^2\n",
    "            P=probability of yes\n",
    "            Q=probability of no\n",
    "            ^=power of\n",
    "            \n",
    "Formula for Entropy:-Plog(P)-Qlog(Q)\n",
    "             P=probability of yes\n",
    "             Q=probability of no\n",
    "             \n",
    "Information Gain for each node = GiniIndex/Entropy(DS->Output variable)-Information*weight(for each node)\n",
    "                    GiniIndex=To find error\n",
    "                    Entropy=To find error\n",
    "                    DS=Output variable\n",
    "          \n",
    "Gini index and Entropy help us to find the error from the given input and output\n",
    "        -In which the error is less then it will be at the top of the table/tree\n",
    "        -In which the error is more the it will be at the bottom of the table/tree\n",
    "        -In which the error is zero the it will not be included in the table/tree\n",
    "        -In which the error is shown little bit also it is included in the table or tree\n",
    "Limitation of DecisionTreeClassifier:-If the column consists of little also information then it will stay with the Tree\n",
    "                and will effect the output. By which the model can become overfit.\n",
    "             \n",
    "Pruning Technique:- to reduced overfitting which occurs through decision tree classifier\n",
    "There are 2 types of Pruning Technique:-\n",
    "    1.max_depth: inbuilt parameter of decision tree classifier class(minlen=1 and maxlen=8)\n",
    "    2.min_samples_leaf:inbuilt parameter of decision tree classifier class(minlen=45 and maxlen=100)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67d6722b",
   "metadata": {},
   "source": [
    "3.ENSEMBLING TECHNIQUE:-training the dataset with multiple algorithms and taking the combined score of all algorithm.\n",
    "                -It should consisits of min=10 DecisionTree and max=100 DecisionTree\n",
    " A.Random Forest Tree (bagging)\n",
    "    There are 2 types of Bootstraping method: 1.bagging and 2. pasting\n",
    " B.Boosting Techniques\n",
    "        1.ADA Boost(Adaptor Boosting):-It works on Decision Stump(min=1 and max=16)\n",
    "                Decision Stump:- It means that it consists of one root node and 2 leaf node\n",
    "                total error=miss classification/total record\n",
    "                initial weight=1/no.of.records\n",
    "                performance of stump=1/2loge(1-total error/total error)\n",
    "                new weight=old weight*e^+-Performance of stump\n",
    "                    + --> stands for wrong predication\n",
    "                    - --> stands for right predication\n",
    "                Normalised Weight=New Weight/Sum(New Weight)\n",
    "        2.Gradient Boosting:-It creates a fully grown tree(min=10 and max=100)\n",
    "                Its basically focus on short comings error  means fully grown tree \n",
    "                error means residual =actual output-predicted output\n",
    "\n",
    "        3.Extreme Gradient Boosting(XGB):-ExtremeGradientClassifier[min=10 , max=100]\n",
    "                -to use this we need to install !pip install xgboost \n",
    "                Extreame Gradient Boosting  : 3rd technique of Boosting \n",
    "                #This is better version of gradient boosting . . short form XGB \n",
    "                #why call better version of Gradient Boosting : -\n",
    "                #1. XG Bossting use : - Multithreading technique\n",
    "                #2. It takes less memory space and faster \n",
    "                #3. It is very useful to handle huge amt of data .Its deal large amount of data\n",
    "                #4. Its handle outlier because have inbuilt capability\n",
    "                #5. Its handle null values \n",
    "                #6. Its handle automatic overfitting situation ."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2122025",
   "metadata": {},
   "source": [
    "4.SUPPORT VECTOR MACHINE(SVM):-It is also used to predict the data\n",
    " -It is also the type of classification algorithm.(means Categorical type of data)\n",
    " -The line which divides into 2 class in SVM is called as Decision Boundary or Hyper Plane\n",
    " -We will select the plane which will give us highest margin/hard margin(bydeafult=hard margin).(margin=d1+d2)\n",
    " -When we add the error at the runtime then the plane will shift the position is called as soft margin\n",
    " -If the data is separated by a straight line it is called as Linear Kernel function.\n",
    " -If the data is not separated by a straight line is called as Polynomial Kernel function.\n",
    " -This is a classification Supervised Learning Algorithm:\n",
    "    There are 3 types of kernel functions :\n",
    "    1. Linear Kernel function (use linear separated data)\n",
    "    2. polynomial kernel function (non-linearly data)\n",
    "    3. radial basis kernel function (non-linearly data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fa571df",
   "metadata": {},
   "source": [
    "5.KNN:- K-nearest Neighbour: It is also an form of Classification algorithm.(min=5 and max=9)\n",
    "       -It will check for closest to the class and fix the point there.\n",
    "       -It will check for highest minimum between the class\n",
    "      \n",
    "       Euclidean distance=rootof(x2-x1)^2+(y2-y1)^2\n",
    "           ^=power of\n",
    "           root of means square of\n",
    "       -It will run only on small amount of data. it will not run on large amount of data.\n",
    "       "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebe1cc95",
   "metadata": {},
   "source": [
    "6.PCA: Principal component analysis : it is used to select the important features from given dataset\n",
    "            -It is also called as feature extraction method.\n",
    "            -It is also called as Dimensionality Reduction Algorithm.\n",
    "            -It will find the variance of all the input and arrange it in the descending order.\n",
    "            -It will add the value of variance and it wil check the sum value of its which will add upto 50% and above \n",
    "                it will take that much features into consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa005d9",
   "metadata": {},
   "source": [
    "# MULTICLASS CLASSIFIER"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f125213d",
   "metadata": {},
   "source": [
    "Multiclass Classifier: means more than 2 classes\n",
    "-it has more then 2 classes in output column\n",
    "-In this type of class we have to check for F1-score\n",
    "-drawback of multiclass is that it will not show you the important feature selection.\n",
    "F1-score : it is called as Harmonic mean of recall and precision.\n",
    "            Harmonic mean(HM) is the reciprocal of airthmetic mean.\n",
    "            formula of harmonic mean for 2 variable A and B=(2*A*B)/(A+B)\n",
    "            A means recall\n",
    "            B means Precision\n",
    "            formula for F1-score=2*recall*precision/(recall/precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
